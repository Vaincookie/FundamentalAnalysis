{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "科研任务.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1vMgXTHPXXkppqOZV95Ad4sDmfG6BjxdE",
      "authorship_tag": "ABX9TyPfMsIcDqmGZTIq4EFkRnMD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaincookie/FundamentalAnalysis/blob/master/%E7%A7%91%E7%A0%94%E4%BB%BB%E5%8A%A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y77KIb--0Iko"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class BP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
        "        # Create the super constructor.\n",
        "        super(BP, self).__init__()\n",
        "        # Get the member variables.\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Create the parameter of the input.\n",
        "        self.W_i = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        # Create the parameter of the hidden.\n",
        "        self.W_h = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        # Create the parameter of the bias_input.\n",
        "        self.B_h = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Create the parameter of the hidden.\n",
        "        self.W_o = nn.Parameter(torch.Tensor(hidden_size, output_size))\n",
        "        # Create the parameter of the bias_output.\n",
        "        self.B_o = nn.Parameter(torch.Tensor(output_size))\n",
        "        # Initialize the parameters.\n",
        "        self.init_weights()\n",
        "\n",
        "    # Initialize the parameters.\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x, init_states=None):\n",
        "\n",
        "        \"\"\"\n",
        "        assumes x.shape represents (batch_size, sequence_size, input_size)\n",
        "        \"\"\"\n",
        "        bs, seq_sz, _ = x.size()\n",
        "        output = []\n",
        "\n",
        "        if init_states is None:\n",
        "            i_t = Variable(torch.zeros(bs, self.hidden_size).to(x.device))\n",
        "            o_t = Variable(torch.zeros(bs, self.output_size).to(x.device))\n",
        "\n",
        "        else:\n",
        "            input_t = init_states\n",
        "            output_t = init_states\n",
        "\n",
        "        for t in range(seq_sz):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            i_t = torch.tanh(x_t @ self.W_i + self.B_h)\n",
        "            o_t = i_t @ self.W_o + self.B_o\n",
        "\n",
        "            output.append(o_t)\n",
        "\n",
        "        Pre_output = output[-1]\n",
        "\n",
        "        return Pre_output\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4Yj4vXi1_oE"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import math\n",
        "from tqdm import trange\n",
        "\n",
        "class ChaoticCNN(nn.Module):\n",
        "    \"\"\"\n",
        "     S_t = tanh(x_t @ W_s1 - B_s)\n",
        "     E_t = sigmoid(L_t @ W_e1 + E_t @ W_e2 - I_t @ W_e3 + S_t @ W_e4 - B_e)\n",
        "     I_t = sigmoid(L_t @ W_i1 - E_t @ W_i2 - I_t @ W_i3 + S_t @ W_i4 - B_i)\n",
        "     L_t = (E_t - I_t)* exp(-K * (S_t) ^2) - S_T\n",
        "     O_t = L_t @ W_o1 - Bo\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_size :int, output_size :int):\n",
        "        # Create the super constructor.\n",
        "        super(ChaoticCNN, self).__init__()\n",
        "        # Get the member variables.\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Create the parameter of the In bias W_s1.\n",
        "        self.W_s1 = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        # Create the parameter of the In bias B_i.\n",
        "        self.B_s = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Create the parameter of the Ex weight e1.\n",
        "        self.W_e1 = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        # Create the parameter of the Ex weight e2.\n",
        "        self.W_e2 = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        # Create the parameter of the Ex weight e3.\n",
        "        self.W_e3 = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        # Create the parameter of the Ex weight e4.\n",
        "        self.W_e4 = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        # Create the parameter of the Ex bias B_e.\n",
        "        self.B_e = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Create the parameter of the In weight i1.\n",
        "        self.W_i1 = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        # Create the parameter of the In weight i2.\n",
        "        self.W_i2 = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        # Create the parameter of the In weight i3.\n",
        "        self.W_i3 = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        # Create the parameter of the In weight i4.\n",
        "        self.W_i4 = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        # Create the parameter of the In bias B_i.\n",
        "        self.B_i = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Create the parameter of the In weight W_o1.\n",
        "        self.W_o1 = nn.Parameter(torch.Tensor(hidden_size, output_size))\n",
        "        # Create the parameter of the In bias B_i.\n",
        "        self.B_o = nn.Parameter(torch.Tensor(output_size))\n",
        "\n",
        "        # Create the parameter of the K.\n",
        "        self.K = nn.Parameter(torch.FloatTensor(1))\n",
        "        # Create the parameter of the N.\n",
        "        self.N = nn.Parameter(torch.FloatTensor(1))\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize the parameters.\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "    def forward(self ,x ,init_states=None):\n",
        "\n",
        "        \"\"\"\n",
        "        assumes x.shape represents (batch_size, sequence_size, input_size)\n",
        "        \"\"\"\n",
        "        bs, seq_sz, _ = x.size()\n",
        "        output = []\n",
        "\n",
        "        if init_states is None:\n",
        "\n",
        "            E_t = Variable(torch.zeros(bs, self.hidden_size).to(x.device))\n",
        "            I_t = Variable(torch.zeros(bs, self.hidden_size).to(x.device))\n",
        "            L_t = Variable(torch.zeros(bs, self.hidden_size).to(x.device))\n",
        "\n",
        "        else:\n",
        "\n",
        "            E_t = init_states\n",
        "            I_t = init_states\n",
        "            L_t = init_states\n",
        "\n",
        "\n",
        "\n",
        "        for t in range(seq_sz):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            S_t = torch.tanh(x_t @ self.W_s1 - self.B_s)\n",
        "            E_t = torch.sigmoid(L_t @ self.W_e1 + E_t @ self.W_e2 - I_t @ self.W_e3 + S_t @ self.W_e4 - self.B_e)\n",
        "            I_t = torch.sigmoid(L_t @ self.W_i1 - E_t @ self.W_i2 - I_t @ self.W_i3 + S_t @ self.W_i4 - self.B_i)\n",
        "            L_t = (E_t - I_t )* torch.exp(- self.K * torch.pow(S_t, 2)) + S_t\n",
        "            O_t = L_t @ self.W_o1 - self.B_o\n",
        "\n",
        "            output.append(O_t.unsqueeze(0))\n",
        "            # reshape hidden_seq p/ retornar\n",
        "        hidden_seq = torch.cat(output, dim=0)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "        Pre_output = hidden_seq[:, -1, :]\n",
        "        return Pre_output, L_t\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CfshuQL2BbH"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
        "        super(GRU, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # z_t\n",
        "        self.W_z = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.U_z = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_z = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # r_t\n",
        "        self.W_r = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.U_r = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_r = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # h_t\n",
        "        self.W_h = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.U_h = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_h = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # o_t\n",
        "        self.U_o = nn.Parameter(torch.Tensor(hidden_size, output_size))\n",
        "        self.b_o = nn.Parameter(torch.Tensor(output_size))\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x, init_states=None):\n",
        "\n",
        "        \"\"\"\n",
        "        assumes x.shape represents (batch_size, sequence_size, input_size)\n",
        "        \"\"\"\n",
        "        bs, seq_sz, _ = x.size()\n",
        "        output = []\n",
        "\n",
        "        if init_states is None:\n",
        "            h_t, g_t, o_t = (\n",
        "                Variable(torch.zeros(bs, self.hidden_size).to(x.device)),\n",
        "                Variable(torch.zeros(bs, self.hidden_size).to(x.device)),\n",
        "                Variable(torch.zeros(bs, self.output_size).to(x.device)),\n",
        "            )\n",
        "        else:\n",
        "            h_t, g_t, o_t = init_states\n",
        "\n",
        "        for t in range(seq_sz):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            z_t = torch.sigmoid(x_t @ self.W_z + h_t @ self.U_z + self.b_z)\n",
        "            r_t = torch.sigmoid(x_t @ self.W_r + h_t @ self.U_r + self.b_r)\n",
        "            g_t = torch.tanh(x_t @ self.W_h + (r_t * h_t) @ self.U_h + self.b_h)\n",
        "            h_t = z_t * h_t + (1 - z_t) * g_t\n",
        "            o_t = h_t @ self.U_o + self.b_o\n",
        "\n",
        "            output.append(o_t.unsqueeze(0))\n",
        "        # reshape hidden_seq p/ retornar\n",
        "        hidden_seq = torch.cat(output, dim=0)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "        Pre_output = hidden_seq[:, -1, :]\n",
        "\n",
        "        return Pre_output, (o_t, h_t)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2VwFG-x2DNa"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # i_t\n",
        "        self.W_i = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.U_i = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_i = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # f_t\n",
        "        self.W_f = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.U_f = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # c_t\n",
        "        self.W_c = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.U_c = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_c = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # o_t\n",
        "        self.W_o = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.U_o = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_o = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # q_t\n",
        "        self.U_q = nn.Parameter(torch.Tensor(hidden_size, output_size))\n",
        "        self.b_q = nn.Parameter(torch.Tensor(output_size))\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x, init_states=None):\n",
        "\n",
        "        \"\"\"\n",
        "        assumes x.shape represents (batch_size, sequence_size, input_size)\n",
        "        \"\"\"\n",
        "        bs, seq_sz, _ = x.size()\n",
        "        output = []\n",
        "\n",
        "        if init_states is None:\n",
        "            c_t, h_t, q_t = (\n",
        "                Variable(torch.zeros(bs, self.hidden_size).to(x.device)),\n",
        "                Variable(torch.zeros(bs, self.hidden_size).to(x.device)),\n",
        "                Variable(torch.zeros(bs, self.output_size).to(x.device)),\n",
        "            )\n",
        "        else:\n",
        "            h_t, c_t = init_states\n",
        "\n",
        "        for t in range(seq_sz):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            i_t = torch.sigmoid(x_t @ self.W_i + h_t @ self.U_i + self.b_i)\n",
        "            f_t = torch.sigmoid(x_t @ self.W_f + h_t @ self.U_f + self.b_f)\n",
        "            o_t = torch.sigmoid(x_t @ self.W_o + h_t @ self.U_o + self.b_o)\n",
        "            g_t = torch.tanh(x_t @ self.W_c + h_t @ self.U_c + self.b_c)\n",
        "            c_t = f_t * c_t + i_t * g_t\n",
        "            h_t = o_t * torch.tanh(c_t)\n",
        "            q_t = h_t @ self.U_q + self.b_q\n",
        "\n",
        "            output.append(q_t.unsqueeze(0))\n",
        "        # reshape hidden_seq p/ retornar\n",
        "        hidden_seq = torch.cat(output, dim=0)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "        Pre_output = hidden_seq[:, -1, :]\n",
        "\n",
        "        return Pre_output, q_t, h_t\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKCnqw0-2FGR"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class RNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size :int):\n",
        "        # Create the super constructor.\n",
        "        super(RNN, self).__init__()\n",
        "        # Get the member variables.\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        # Create the parameter of the input.\n",
        "        self.W_i = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        # Create the parameter of the hidden.\n",
        "        self.W_h = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        # Create the parameter of the bias_input.\n",
        "        self.B_h = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Create the parameter of the hidden.\n",
        "        self.W_o = nn.Parameter(torch.Tensor(hidden_size, output_size))\n",
        "        # Create the parameter of the bias_output.\n",
        "        self.B_o = nn.Parameter(torch.Tensor(output_size))\n",
        "        # Initialize the parameters.\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize the parameters.\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "    def forward(self ,x ,init_states=None):\n",
        "\n",
        "        \"\"\"\n",
        "        assumes x.shape represents (batch_size, sequence_size, input_size)\n",
        "        \"\"\"\n",
        "        bs, seq_sz, _ = x.size()\n",
        "        # output = torch.zeros(bs, self.output_size).to(x.device)\n",
        "        output = list()\n",
        "\n",
        "        if init_states is None:\n",
        "            i_t = Variable(torch.zeros(bs, self.hidden_size).to(x.device))\n",
        "            h_t = Variable(torch.zeros(bs, self.output_size).to(x.device))\n",
        "\n",
        "        else:\n",
        "            i_t = init_states\n",
        "            h_t = init_states\n",
        "\n",
        "        for t in range(seq_sz):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            i_t = torch.tanh(x_t @ self.W_i + i_t @ self.W_h + self.B_h)\n",
        "            h_t = i_t @ self.W_o + self.B_o\n",
        "\n",
        "            output.append(h_t.unsqueeze(0))\n",
        "        # reshape hidden_seq p/ retornar\n",
        "        hidden_seq = torch.cat(output, dim=0)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "        Pre_output = hidden_seq[:, -1, :]\n",
        "\n",
        "        return Pre_output, h_t\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNUnxax90PVs",
        "outputId": "bc4618c7-a2bf-4d88-865a-955b2398c099"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dRv-rNO0sE5",
        "outputId": "2457fe6c-591d-4493-ce3c-9c87af92894f"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import math\n",
        "from tqdm import trange\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "new=pd.DataFrame({'product':product,\n",
        "                  'model':model_name,\n",
        "                  'epoch':epoch,\n",
        "                  'learning rate':lr,\n",
        "                  'hidden_size':hidden_size,\n",
        "                  'input_size':input_size,\n",
        "                  'time_steps':time_steps,\n",
        "                  'mean_squared_error':mse,\n",
        "                  'root_mean_squared_error':rmse,\n",
        "                  'mean_absolute_error':mae,\n",
        "                  },index=[0])   # 自定义索引为：1 ，这里也可以不设置index\n",
        "new.to_csv(\"new_record_exp.csv\")\n",
        "product_list = [\"AUS200\",\"CHN50\",\"ESP35\",\"EUSTX50\",\"FRA40\",\"GBPUSD\",\"GER30\",\"HKG33\",\"JPN225\",\"NAS100\",\"SPX500\",\"UK100\",\"US30\",\"US2000\",\"USDCNH\",\"USDEUR\"]\n",
        "product = \"JPN225\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for par1 in [64,128,256,512]:\n",
        "  for par2 in [1e-4,1e-5]:\n",
        "    for par3 in [3,5,10,15,31]:\n",
        "      for product_item in product_list:\n",
        "        product = product_item\n",
        "        data = pd.read_csv('/content/drive/MyDrive/Research_essay/'+product+\".csv\")\n",
        "        # 设置随机种子\n",
        "        torch.manual_seed(1)\n",
        "        input_size = 9\n",
        "        hidden_size = par1\n",
        "        output_size = 1\n",
        "        lr = par2\n",
        "        dim_in = 9\n",
        "        dim_out = 1\n",
        "        time_steps = par3\n",
        "        smoothing_window_size = 10000\n",
        "        epoch = 500\n",
        "\n",
        "        # 定义训练集\n",
        "        data = data.sort_values(['trade_date'], ascending=True)\n",
        "        data = data[['ts_code', 'trade_date', 'bid_close', 'bid_open', 'bid_high', 'bid_low',\n",
        "              'ask_open', 'ask_close', 'ask_high', 'ask_low', 'tick_qty']]\n",
        "        data = data.sort_values(['trade_date'], ascending=True)\n",
        "        data = data.drop(labels=[\"ts_code\",'trade_date'], axis=1)\n",
        "        train_data = data[int(data.shape[0]*0.2):]\n",
        "        test_data = data[:int(data.shape[0] * 0.2)]\n",
        "\n",
        "        scaler = MinMaxScaler()\n",
        "        train_data = train_data.values.astype('float32')\n",
        "        test_data = test_data.values.astype('float32')\n",
        "\n",
        "\n",
        "        train_len = len(train_data)\n",
        "        remainder = int(train_len) % smoothing_window_size\n",
        "        for di in range(0, int(train_len) - remainder, smoothing_window_size):\n",
        "            scaler.fit(train_data[di:di + smoothing_window_size, :])\n",
        "            train_data[di:di + smoothing_window_size, :] = scaler.transform(train_data[di:di + smoothing_window_size, :])\n",
        "\n",
        "        di = int(train_len) - remainder\n",
        "        if (remainder != 0):\n",
        "            scaler.fit(train_data[di:, :])\n",
        "            train_data[di:, :] = scaler.transform(train_data[di:, :])\n",
        "        test_data = scaler.transform(test_data)\n",
        "\n",
        "\n",
        "        # 根据原始数据集构建矩阵\n",
        "        def create_dataset(data, time_steps):\n",
        "            dataX, dataY = [], []\n",
        "            for i in range(len(data) - time_steps):\n",
        "                a = data[i:(i + time_steps), :]\n",
        "                dataX.append(a)\n",
        "                dataY.append(data[i + time_steps, 0:1])\n",
        "            return np.array(dataX), np.array(dataY)\n",
        "\n",
        "        trainX, trainY = create_dataset(train_data, time_steps)\n",
        "        testX, testY = create_dataset(test_data, time_steps)\n",
        "\n",
        "        trainX = torch.from_numpy(trainX).to(torch.float32)\n",
        "        trainY = torch.from_numpy(trainY).to(torch.float32)\n",
        "        testX = torch.from_numpy(testX).to(torch.float32)\n",
        "        testY = torch.from_numpy(testY).to(torch.float32)\n",
        "\n",
        "\n",
        "        for par in [1,2,3,4,5]:\n",
        "            model_select = par\n",
        "            if model_select == 1:\n",
        "                model_name = \"BP\"\n",
        "                model = BP(input_size, hidden_size, output_size)\n",
        "            if model_select == 2:\n",
        "                model_name = \"LSTM\"\n",
        "                model = LSTM(input_size, hidden_size, output_size)\n",
        "            if model_select == 3:\n",
        "                model_name = \"RNN\"\n",
        "                model = RNN(input_size, hidden_size, output_size)\n",
        "            if model_select == 4:\n",
        "                model_name = \"GRU\"\n",
        "                model = GRU(input_size, hidden_size, output_size)\n",
        "            if model_select == 5:\n",
        "                model_name = \"CCNN\"\n",
        "                model = ChaoticCNN(input_size, hidden_size, output_size)\n",
        "\n",
        "            bp = model\n",
        "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "            bp = bp.to(device)\n",
        "            trainx = trainX.to(device)\n",
        "            trainy = trainY.to(device)\n",
        "            testx = testX.to(device)\n",
        "            testy = testY.to(device)\n",
        "            loss_function = nn.MSELoss()\n",
        "            optimizer = optim.Adam(bp.parameters(), lr)\n",
        "\n",
        "            list_x = []\n",
        "            list_y = []\n",
        "            list_ax = []\n",
        "            list_ay = []\n",
        "\n",
        "            begin = time.time()\n",
        "\n",
        "            for epoch in range(epoch):\n",
        "\n",
        "                init_states = None\n",
        "                if(model_select == 1):\n",
        "                    output = bp(trainx, init_states)\n",
        "                elif(model_select == 2):\n",
        "                    output, ht ,qt= bp(trainx, init_states)\n",
        "                else:\n",
        "                    output, ht = bp(trainx, init_states)\n",
        "                loss = loss_function(output, trainy)\n",
        "                running_loss = loss.item()\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                Loss = []\n",
        "\n",
        "                list_x.append(epoch)\n",
        "                list_y.append(loss.item())\n",
        "\n",
        "                if epoch == 500:\n",
        "                    list_ax.append(epoch)\n",
        "                    list_ay.append(loss.item())\n",
        "                    plt.plot(list_x, list_y, color='r', linewidth=3)\n",
        "                    plt.scatter(list_ax, list_ay, marker='*', color='r')\n",
        "                    plt.show()\n",
        "\n",
        "                if (epoch + 1) % 100 == 0:\n",
        "                    Loss.append(running_loss)\n",
        "                    print('Epoch: {}, Loss:{:.5f}, learning rate:{}'.format(epoch + 1, loss.item(), 1e-4))\n",
        "\n",
        "            pass\n",
        "\n",
        "            time_elapsed = time.time() - begin\n",
        "            print('Training Complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "            bp = bp.eval()\n",
        "            init_states = None\n",
        "            if (model_select == 1):\n",
        "                pre_output = bp(testx, init_states)\n",
        "            elif (model_select == 2):\n",
        "                pre_output,pre_qt,pre_ht = bp(testx, init_states)\n",
        "            else:\n",
        "                pre_output,pre_ht = bp(testx, init_states)\n",
        "                # pre_qt = pre_qt.cpu()\n",
        "                # pre_ht = pre_ht.cpu()\n",
        "                #ht = ht.cpu()\n",
        "\n",
        "            output = output.cpu()\n",
        "            trainx = trainX.cpu()\n",
        "            trainy = trainY.cpu()\n",
        "            testx = testX.cpu()\n",
        "            testy = testY.cpu()\n",
        "            pre_output = pre_output.cpu()\n",
        "\n",
        "\n",
        "\n",
        "            pred = pre_output.data.numpy().reshape(testx.shape[0], 1)\n",
        "            real = testy.reshape(testx.shape[0], 1)\n",
        "\n",
        "            mse = mean_squared_error(pred, real)\n",
        "            rmse = math.sqrt(mean_squared_error(pred, real))\n",
        "            mae = mean_absolute_error(pred, real)\n",
        "            print('mean_squared_error:%.6f' % mse)\n",
        "            print('root_mean_squared_error:%.6f' % rmse)\n",
        "            print('mean_absolute_error:%.6f' % mae)\n",
        "\n",
        "\n",
        "            record = pd.read_csv(\"/content/drive/MyDrive/Research_essay/new_record_exp.csv\",index_col=0)\n",
        "            record = record.append([{'product':product,\n",
        "                              'model':model_name,\n",
        "                              'epoch':epoch,\n",
        "                              'learning rate':lr,\n",
        "                              'hidden_size':hidden_size,\n",
        "                              'input_size':input_size,\n",
        "                              'time_steps':time_steps,            \n",
        "                              'mean_squared_error':mse,\n",
        "                              'root_mean_squared_error':rmse,\n",
        "                              'mean_absolute_error':mae,\n",
        "                                'time':time_elapsed,\n",
        "\n",
        "                              }],ignore_index=True)\n",
        "            record.to_csv(\"/content/drive/MyDrive/Research_essay/new_record_exp.csv\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 100, Loss:0.08466, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.01489, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00739, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00675, learning rate:0.0001\n",
            "Epoch: 500, Loss:0.00627, learning rate:0.0001\n",
            "Training Complete in 0m 1s\n",
            "mean_squared_error:0.016221\n",
            "root_mean_squared_error:0.127363\n",
            "mean_absolute_error:0.096953\n",
            "Epoch: 100, Loss:0.10911, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00352, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00104, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00097, learning rate:0.0001\n",
            "Training Complete in 0m 3s\n",
            "mean_squared_error:0.003788\n",
            "root_mean_squared_error:0.061543\n",
            "mean_absolute_error:0.041463\n",
            "Epoch: 100, Loss:0.01147, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00721, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00508, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00337, learning rate:0.0001\n",
            "Training Complete in 0m 1s\n",
            "mean_squared_error:0.003871\n",
            "root_mean_squared_error:0.062215\n",
            "mean_absolute_error:0.048069\n",
            "Epoch: 100, Loss:0.10687, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00244, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00181, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00160, learning rate:0.0001\n",
            "Training Complete in 0m 3s\n",
            "mean_squared_error:0.004074\n",
            "root_mean_squared_error:0.063829\n",
            "mean_absolute_error:0.044937\n",
            "Epoch: 100, Loss:0.01597, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00551, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00101, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00049, learning rate:0.0001\n",
            "Training Complete in 0m 4s\n",
            "mean_squared_error:0.000752\n",
            "root_mean_squared_error:0.027419\n",
            "mean_absolute_error:0.020136\n",
            "Epoch: 100, Loss:0.02503, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.01318, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00842, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00493, learning rate:0.0001\n",
            "Epoch: 500, Loss:0.00262, learning rate:0.0001\n",
            "Training Complete in 0m 1s\n",
            "mean_squared_error:0.010061\n",
            "root_mean_squared_error:0.100304\n",
            "mean_absolute_error:0.086729\n",
            "Epoch: 100, Loss:0.03192, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00363, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00194, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00118, learning rate:0.0001\n",
            "Training Complete in 0m 3s\n",
            "mean_squared_error:0.005337\n",
            "root_mean_squared_error:0.073057\n",
            "mean_absolute_error:0.059530\n",
            "Epoch: 100, Loss:0.01378, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00325, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00055, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00037, learning rate:0.0001\n",
            "Training Complete in 0m 1s\n",
            "mean_squared_error:0.001646\n",
            "root_mean_squared_error:0.040567\n",
            "mean_absolute_error:0.028919\n",
            "Epoch: 100, Loss:0.03076, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00380, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00198, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00099, learning rate:0.0001\n",
            "Training Complete in 0m 3s\n",
            "mean_squared_error:0.002786\n",
            "root_mean_squared_error:0.052782\n",
            "mean_absolute_error:0.045352\n",
            "Epoch: 100, Loss:0.00386, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00024, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00024, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00024, learning rate:0.0001\n",
            "Training Complete in 0m 4s\n",
            "mean_squared_error:0.000767\n",
            "root_mean_squared_error:0.027688\n",
            "mean_absolute_error:0.019156\n",
            "Epoch: 100, Loss:0.06071, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.01266, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00802, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00707, learning rate:0.0001\n",
            "Epoch: 500, Loss:0.00620, learning rate:0.0001\n",
            "Training Complete in 0m 1s\n",
            "mean_squared_error:0.060636\n",
            "root_mean_squared_error:0.246243\n",
            "mean_absolute_error:0.211417\n",
            "Epoch: 100, Loss:0.08397, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00338, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00153, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00139, learning rate:0.0001\n",
            "Training Complete in 0m 3s\n",
            "mean_squared_error:0.004322\n",
            "root_mean_squared_error:0.065740\n",
            "mean_absolute_error:0.055098\n",
            "Epoch: 100, Loss:0.01152, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00714, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00430, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00247, learning rate:0.0001\n",
            "Training Complete in 0m 1s\n",
            "mean_squared_error:0.026990\n",
            "root_mean_squared_error:0.164286\n",
            "mean_absolute_error:0.135787\n",
            "Epoch: 100, Loss:0.08211, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00311, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00240, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00200, learning rate:0.0001\n",
            "Training Complete in 0m 3s\n",
            "mean_squared_error:0.010272\n",
            "root_mean_squared_error:0.101350\n",
            "mean_absolute_error:0.086108\n",
            "Epoch: 100, Loss:0.01370, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00308, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00078, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00071, learning rate:0.0001\n",
            "Training Complete in 0m 4s\n",
            "mean_squared_error:0.004383\n",
            "root_mean_squared_error:0.066204\n",
            "mean_absolute_error:0.054848\n",
            "Epoch: 100, Loss:0.05371, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00993, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00606, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00544, learning rate:0.0001\n",
            "Epoch: 500, Loss:0.00492, learning rate:0.0001\n",
            "Training Complete in 0m 1s\n",
            "mean_squared_error:0.019339\n",
            "root_mean_squared_error:0.139065\n",
            "mean_absolute_error:0.129609\n",
            "Epoch: 100, Loss:0.08000, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00310, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00125, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00115, learning rate:0.0001\n",
            "Training Complete in 0m 2s\n",
            "mean_squared_error:0.002918\n",
            "root_mean_squared_error:0.054017\n",
            "mean_absolute_error:0.045065\n",
            "Epoch: 100, Loss:0.00903, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00611, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00418, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00281, learning rate:0.0001\n",
            "Training Complete in 0m 1s\n",
            "mean_squared_error:0.003402\n",
            "root_mean_squared_error:0.058330\n",
            "mean_absolute_error:0.052635\n",
            "Epoch: 100, Loss:0.07765, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00259, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00198, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00169, learning rate:0.0001\n",
            "Training Complete in 0m 2s\n",
            "mean_squared_error:0.003921\n",
            "root_mean_squared_error:0.062615\n",
            "mean_absolute_error:0.055062\n",
            "Epoch: 100, Loss:0.01125, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00385, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00109, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00080, learning rate:0.0001\n",
            "Training Complete in 0m 3s\n",
            "mean_squared_error:0.000321\n",
            "root_mean_squared_error:0.017907\n",
            "mean_absolute_error:0.013321\n",
            "Epoch: 100, Loss:0.03785, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.01287, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00977, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00786, learning rate:0.0001\n",
            "Epoch: 500, Loss:0.00611, learning rate:0.0001\n",
            "Training Complete in 0m 1s\n",
            "mean_squared_error:0.008205\n",
            "root_mean_squared_error:0.090582\n",
            "mean_absolute_error:0.075245\n",
            "Epoch: 100, Loss:0.05463, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00307, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00162, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00125, learning rate:0.0001\n",
            "Training Complete in 0m 3s\n",
            "mean_squared_error:0.001869\n",
            "root_mean_squared_error:0.043230\n",
            "mean_absolute_error:0.032336\n",
            "Epoch: 100, Loss:0.01374, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00640, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00232, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00081, learning rate:0.0001\n",
            "Training Complete in 0m 1s\n",
            "mean_squared_error:0.000643\n",
            "root_mean_squared_error:0.025352\n",
            "mean_absolute_error:0.019746\n",
            "Epoch: 100, Loss:0.05274, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00337, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00236, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00164, learning rate:0.0001\n",
            "Training Complete in 0m 3s\n",
            "mean_squared_error:0.001825\n",
            "root_mean_squared_error:0.042720\n",
            "mean_absolute_error:0.034067\n",
            "Epoch: 100, Loss:0.00972, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00049, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00030, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00030, learning rate:0.0001\n",
            "Training Complete in 0m 4s\n",
            "mean_squared_error:0.000434\n",
            "root_mean_squared_error:0.020832\n",
            "mean_absolute_error:0.015587\n",
            "Epoch: 100, Loss:0.06330, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.01851, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.01241, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00960, learning rate:0.0001\n",
            "Epoch: 500, Loss:0.00720, learning rate:0.0001\n",
            "Training Complete in 0m 1s\n",
            "mean_squared_error:0.024456\n",
            "root_mean_squared_error:0.156384\n",
            "mean_absolute_error:0.108374\n",
            "Epoch: 100, Loss:0.07876, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00374, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00171, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00124, learning rate:0.0001\n",
            "Training Complete in 0m 3s\n",
            "mean_squared_error:0.001705\n",
            "root_mean_squared_error:0.041292\n",
            "mean_absolute_error:0.032455\n",
            "Epoch: 100, Loss:0.01823, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00765, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00253, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00083, learning rate:0.0001\n",
            "Training Complete in 0m 1s\n",
            "mean_squared_error:0.008374\n",
            "root_mean_squared_error:0.091510\n",
            "mean_absolute_error:0.060633\n",
            "Epoch: 100, Loss:0.07709, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00479, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00303, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00187, learning rate:0.0001\n",
            "Training Complete in 0m 3s\n",
            "mean_squared_error:0.003001\n",
            "root_mean_squared_error:0.054782\n",
            "mean_absolute_error:0.041634\n",
            "Epoch: 100, Loss:0.01416, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00066, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00033, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00032, learning rate:0.0001\n",
            "Training Complete in 0m 4s\n",
            "mean_squared_error:0.002400\n",
            "root_mean_squared_error:0.048988\n",
            "mean_absolute_error:0.035791\n",
            "Epoch: 100, Loss:0.05290, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.01596, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.01136, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00912, learning rate:0.0001\n",
            "Epoch: 500, Loss:0.00710, learning rate:0.0001\n",
            "Training Complete in 0m 1s\n",
            "mean_squared_error:0.017942\n",
            "root_mean_squared_error:0.133949\n",
            "mean_absolute_error:0.129973\n",
            "Epoch: 100, Loss:0.06907, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00330, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00157, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00122, learning rate:0.0001\n",
            "Training Complete in 0m 3s\n",
            "mean_squared_error:0.003526\n",
            "root_mean_squared_error:0.059378\n",
            "mean_absolute_error:0.052152\n",
            "Epoch: 100, Loss:0.01545, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00674, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00232, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00073, learning rate:0.0001\n",
            "Training Complete in 0m 1s\n",
            "mean_squared_error:0.000177\n",
            "root_mean_squared_error:0.013299\n",
            "mean_absolute_error:0.009679\n",
            "Epoch: 100, Loss:0.06721, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00344, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00233, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00160, learning rate:0.0001\n",
            "Training Complete in 0m 3s\n",
            "mean_squared_error:0.003031\n",
            "root_mean_squared_error:0.055058\n",
            "mean_absolute_error:0.050421\n",
            "Epoch: 100, Loss:0.01223, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00040, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00020, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00019, learning rate:0.0001\n",
            "Training Complete in 0m 4s\n",
            "mean_squared_error:0.000115\n",
            "root_mean_squared_error:0.010728\n",
            "mean_absolute_error:0.008010\n",
            "Epoch: 100, Loss:0.05908, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00921, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00485, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00451, learning rate:0.0001\n",
            "Epoch: 500, Loss:0.00422, learning rate:0.0001\n",
            "Training Complete in 0m 1s\n",
            "mean_squared_error:0.013047\n",
            "root_mean_squared_error:0.114224\n",
            "mean_absolute_error:0.098247\n",
            "Epoch: 100, Loss:0.08662, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00253, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00062, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00058, learning rate:0.0001\n",
            "Training Complete in 0m 3s\n",
            "mean_squared_error:0.002017\n",
            "root_mean_squared_error:0.044915\n",
            "mean_absolute_error:0.036582\n",
            "Epoch: 100, Loss:0.00721, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00510, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00378, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00264, learning rate:0.0001\n",
            "Training Complete in 0m 1s\n",
            "mean_squared_error:0.004158\n",
            "root_mean_squared_error:0.064486\n",
            "mean_absolute_error:0.055658\n",
            "Epoch: 100, Loss:0.08407, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00157, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00114, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00103, learning rate:0.0001\n",
            "Training Complete in 0m 2s\n",
            "mean_squared_error:0.002934\n",
            "root_mean_squared_error:0.054164\n",
            "mean_absolute_error:0.045462\n",
            "Epoch: 100, Loss:0.01064, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.00452, learning rate:0.0001\n",
            "Epoch: 300, Loss:0.00107, learning rate:0.0001\n",
            "Epoch: 400, Loss:0.00033, learning rate:0.0001\n",
            "Training Complete in 0m 4s\n",
            "mean_squared_error:0.000480\n",
            "root_mean_squared_error:0.021905\n",
            "mean_absolute_error:0.015262\n",
            "Epoch: 100, Loss:0.04536, learning rate:0.0001\n",
            "Epoch: 200, Loss:0.01831, learning rate:0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-zDxBp4lYIV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}