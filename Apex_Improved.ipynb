{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Apex.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPjxdIfmAuAXJBI2za+nSP2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaincookie/FundamentalAnalysis/blob/master/Apex_Improved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBeiMvfKGBUu"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import trange\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data \n",
        "\n",
        "train= pd.read_csv('/content/train.csv')\n",
        "test= pd.read_csv('/content/test.csv')\n",
        "test_ids= test['id'].to_numpy()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm2XO2OWxtBG"
      },
      "source": [
        "from apex import amp"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVnZitCrG2Ws"
      },
      "source": [
        "def preprocess(df):\n",
        "    #some new\n",
        "    dfr= pd.get_dummies(df['R'], prefix= \"R_\")\n",
        "    df= pd.concat([df, dfr], axis= 1)\n",
        "    dfc= pd.get_dummies(df['C'], prefix= \"C_\")\n",
        "    df= pd.concat([df, dfc], axis= 1)\n",
        "    df= df.drop(['R', 'C'], axis= 1)\n",
        "\n",
        "    df['u_in_cumsum']= df['u_in'].groupby(df['breath_id']).cumsum()\n",
        "    df['time_step_cumsum']= df['time_step'].groupby(df['breath_id']).cumsum()\n",
        "    \n",
        "    df['u_in_min']= df['u_in'].groupby(df['breath_id']).transform('min')\n",
        "    df['u_in_max']= df['u_in'].groupby(df['breath_id']).transform('max')\n",
        "    df['u_in_mean']= df['u_in'].groupby(df['breath_id']).transform('mean')\n",
        "   \n",
        "    df['u_in_lag2']= df['u_in'].groupby(df['breath_id']).shift(2)\n",
        "    df['u_in_lag1']= df['u_in'].groupby(df['breath_id']).shift(1)\n",
        "    df['u_in_lag-1']= df['u_in'].groupby(df['breath_id']).shift(-1)\n",
        "    df['u_in_lag-2']= df['u_in'].groupby(df['breath_id']).shift(-2)\n",
        "    df= df.fillna(0)\n",
        "\n",
        "    df['u_in_diff1']= df['u_in']- df['u_in_lag1']\n",
        "    df['u_in_diff2']= df['u_in']- df['u_in_lag2']\n",
        "    df['u_in_diff3']= df['u_in_max']- df['u_in']\n",
        "    df['u_in_diff4']= df['u_in_mean']- df['u_in']\n",
        "\n",
        "    df1= df[df['u_out'] == 0]\n",
        "    df['mean_inspiratory_uin']= df1['u_in'].groupby(df['breath_id']).transform('mean')\n",
        "\n",
        "    df2= df[df['u_out'] == 1]\n",
        "    df['mean_expiratory_uin']= df2['u_in'].groupby(df['breath_id']).transform('mean')\n",
        "    \n",
        "    df['u_in_diff5']= df['mean_inspiratory_uin']- df['u_in']\n",
        "    df['u_in_diff6']= df['mean_expiratory_uin']- df['u_in']\n",
        "    \n",
        "    df= df.fillna(0)\n",
        "    \n",
        "    df['delta_t']= df.groupby('breath_id')['time_step'].diff().fillna(0)\n",
        "    df['delta_uin']= df.groupby('breath_id')['u_in'].diff().fillna(0)\n",
        "    \n",
        "    df['area']= df['u_in']*df['delta_t']\n",
        "    df['area']= df.groupby('breath_id')['area'].cumsum()\n",
        "    df['slope']= (df['delta_uin']/df['delta_t']).fillna(0)\n",
        "\n",
        "\n",
        "\n",
        "    return df"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0R1mjgoG407"
      },
      "source": [
        "groups= train.breath_id.values.reshape(-1, 80)[:, 0]\n",
        "groups.shape\n",
        "\n",
        "train= preprocess(train)\n",
        "targets= train['pressure'].to_numpy().reshape(-1, 80)\n",
        "train.drop(['id','pressure', \"breath_id\"], axis= 1, inplace= True)\n",
        "\n",
        "test= preprocess(test)\n",
        "test.drop(['id', \"breath_id\"], axis= 1, inplace= True)\n",
        "y_test= np.zeros(test.shape[0]).reshape(-1, 80)\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "RS = RobustScaler()\n",
        "train = RS.fit_transform(train)\n",
        "test  = RS.transform(test)\n",
        "\n",
        "num_features= train.shape[-1]\n",
        "train= train.reshape(-1, 80, num_features)\n",
        "test= test.reshape(-1, 80, num_features)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD9C9PU-G6xb"
      },
      "source": [
        "class CustomDataset:\n",
        "    def __init__(self, data, target):\n",
        "        self.data= data\n",
        "        self.target= target\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        current_sample= self.data[idx, :, :]\n",
        "        current_target= self.target[idx, :]\n",
        "        \n",
        "        return torch.tensor(current_sample, dtype= torch.float), torch.tensor(current_target, dtype= torch.float)\n",
        "     "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsiq1QJ3G9wH"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(RNNModel, self).__init__()\n",
        "        \n",
        "        hidden_dim= [400, 300, 200, 100]\n",
        "        self.bilstm1= nn.LSTM(input_dim, hidden_dim[0], batch_first= True, bidirectional= True)\n",
        "        self.norm1= nn.LayerNorm(hidden_dim[0]*2)\n",
        "        \n",
        "        self.bilstm2= nn.LSTM(hidden_dim[0]*2, hidden_dim[1], batch_first= True, bidirectional= True)\n",
        "        self.norm2= nn.LayerNorm(hidden_dim[1]*2)\n",
        "        \n",
        "        self.bilstm3= nn.LSTM(hidden_dim[1]*2, hidden_dim[2], batch_first= True, bidirectional= True)\n",
        "        self.norm3= nn.LayerNorm(hidden_dim[2]*2)\n",
        "        \n",
        "        self.bilstm4= nn.LSTM(hidden_dim[2]*2, hidden_dim[3], batch_first= True, bidirectional= True)\n",
        "        self.norm4= nn.LayerNorm(hidden_dim[3]*2)\n",
        "        \n",
        "        self.d= nn.Dropout(p= 0.002)\n",
        "        \n",
        "        self.fc1= nn.Linear(hidden_dim[3]*2, 100)\n",
        "        self.fc2= nn.Linear(100, output_dim)\n",
        "#         self.fc3= nn.Linear(32, output_dim)\n",
        "\n",
        "        \n",
        "    def forward(self, X):\n",
        "        pred, _= self.bilstm1(X)\n",
        "        pred= self.norm1(pred)\n",
        "        \n",
        "        pred, _= self.bilstm2(pred)\n",
        "        pred= self.norm2(pred)\n",
        "        \n",
        "        pred, _= self.bilstm3(pred)\n",
        "        pred= self.norm3(pred)\n",
        "        \n",
        "        pred, _= self.bilstm4(pred)\n",
        "        pred= self.norm4(pred)\n",
        "        \n",
        "        pred= self.d(pred)\n",
        "        \n",
        "        pred= self.fc1(pred)\n",
        "        pred= F.selu(pred)\n",
        "        \n",
        "        pred= self.fc2(pred)\n",
        "        \n",
        "#         pred= F.selu(pred)\n",
        "#         pred= self.fc3(pred)\n",
        "\n",
        "        pred= pred.squeeze(dim= 2)\n",
        "        \n",
        "        return pred"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B42ndDxqHABf"
      },
      "source": [
        "def initialize_parameters(m):\n",
        "    if isinstance(m, nn.LSTM):\n",
        "        nn.init.orthogonal_(m.weight_ih_l0.data, gain= nn.init.calculate_gain('tanh'))\n",
        "        nn.init.orthogonal_(m.weight_hh_l0.data, gain= nn.init.calculate_gain('tanh'))\n",
        "        nn.init.orthogonal_(m.weight_ih_l0_reverse.data, gain= nn.init.calculate_gain('tanh'))\n",
        "        nn.init.orthogonal_(m.weight_hh_l0_reverse.data, gain= nn.init.calculate_gain('tanh'))\n",
        "        \n",
        "        nn.init.constant_(m.bias_ih_l0.data, 0)\n",
        "        nn.init.constant_(m.bias_hh_l0.data, 0)\n",
        "        nn.init.constant_(m.bias_ih_l0_reverse.data, 0)\n",
        "        nn.init.constant_(m.bias_hh_l0_reverse.data, 0)\n",
        "        \n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight.data)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2wybfiRHBYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf2b364-0e7a-4ab9-c861-2c090847b89e"
      },
      "source": [
        "device= \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "INPUT_DIM= num_features\n",
        "OUTPUT_DIM= 1\n",
        "BATCH_SIZE= 1024\n",
        "\n",
        "model= RNNModel(input_dim= INPUT_DIM, output_dim= OUTPUT_DIM).to(device)\n",
        "model.apply(initialize_parameters)\n",
        "\n",
        "criterion= nn.L1Loss()\n",
        "criterion.to(device)\n",
        "\n",
        "optimizer= optim.Adam(model.parameters(), lr= 0.001)\n",
        "\n",
        "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") # 这里是“欧一”，不是“零一”\n",
        "\n",
        "scheduler= optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor= 0.5, patience= 10, verbose= True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVaZ-At0HDMr"
      },
      "source": [
        "\n",
        "def train_model(dataloader, model, criterion, optimizer):\n",
        "    size= len(dataloader.dataset)\n",
        "    model.train()\n",
        "    batches= len(dataloader)\n",
        "    train_loss= 0\n",
        "    \n",
        "    for batch_idx, (X, y) in enumerate(dataloader):\n",
        "        X, y= X.to(device), y.to(device)\n",
        "\n",
        "        scores= model(X)\n",
        "        loss= criterion(scores, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "          scaled_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss= loss.item()\n",
        "        train_loss += loss\n",
        "        \n",
        "    train_loss_avg= train_loss/batches\n",
        "    print(f\"avg. train loss: {train_loss_avg}\")\n",
        "    \n",
        "    return train_loss_avg"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFfEFzzFHEwD"
      },
      "source": [
        "def val_model(dataloader, model, criterion):\n",
        "    \n",
        "    size= len(dataloader.dataset)\n",
        "    batches= len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss= 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in (dataloader):\n",
        "            X, y= X.to(device), y.to(device)\n",
        "      \n",
        "            scores= model(X)\n",
        "            test_loss += criterion(scores, y)\n",
        "\n",
        "    test_loss /= batches\n",
        "\n",
        "    print(f\"avg test loss : {test_loss}\")\n",
        "    \n",
        "    return test_loss"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oO8dweLHF_H"
      },
      "source": [
        "def predict_model(dataloader, model):\n",
        "    model.eval()\n",
        "    y_pred= np.array([])\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X , y in dataloader:\n",
        "            X, y= X.to(device), y.to(device)\n",
        "            \n",
        "            preds= model(X)\n",
        "            preds= preds.flatten().cpu().numpy()\n",
        "            \n",
        "            y_pred= np.concatenate((y_pred, preds))\n",
        "            \n",
        "    return y_pred"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVk82GdIHHnn",
        "outputId": "3ab69ff7-f29b-4bc3-c176-17344a2cc82f"
      },
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "kfold= GroupKFold(n_splits= 5)\n",
        "EPOCHS= 200\n",
        "cv_scores= []\n",
        "predictions= np.zeros(test_ids.shape[0])\n",
        "\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(train, targets, groups= groups)):\n",
        "    X_train, X_val= train[train_idx], train[val_idx]\n",
        "    y_train, y_val= targets[train_idx], targets[val_idx]\n",
        "    \n",
        "    train_dataset= CustomDataset(data= X_train, target= y_train)\n",
        "    val_dataset= CustomDataset(data= X_val, target= y_val)\n",
        "\n",
        "    train_loader= data.DataLoader(train_dataset, batch_size= BATCH_SIZE)\n",
        "    val_loader= data.DataLoader(val_dataset, batch_size= BATCH_SIZE)\n",
        "    \n",
        "    best_valid_loss= float('inf')\n",
        "    \n",
        "    avg_train_losses= []\n",
        "    avg_val_losses= []\n",
        "    \n",
        "    for t in trange(EPOCHS):\n",
        "        print(f\"Epoch: {t+1}\")\n",
        "        train_loss= train_model(train_loader, model, criterion, optimizer)\n",
        "        val_loss= val_model(val_loader, model, criterion)\n",
        "        \n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_val_losses.append(val_loss)\n",
        "        \n",
        "        if (val_loss< best_valid_loss):\n",
        "            best_valid_loss= val_loss\n",
        "            ofilename = 'ventilator%d.pth' % fold\n",
        "            torch.save(model.state_dict(),  ofilename)\n",
        "        \n",
        "        scheduler.step(val_loss)\n",
        "    \n",
        "    cv_scores.append(best_valid_loss)\n",
        "    \n",
        "    test_dataset= CustomDataset(data= test, target= y_test)\n",
        "    test_loader= data.DataLoader(test_dataset, batch_size= BATCH_SIZE)\n",
        "                       \n",
        "    model.load_state_dict(torch.load('ventilator%d.pth' % fold, map_location=device))\n",
        "    predictions += (predict_model(test_loader, model))\n",
        "    \n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/200 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "avg. train loss: 1.8858564112146021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/200 [00:36<1:59:25, 36.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg test loss : 0.7619773149490356\n",
            "Epoch: 2\n",
            "avg. train loss: 0.673204975613093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 2/200 [01:11<1:58:06, 35.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg test loss : 0.5787755846977234\n",
            "Epoch: 3\n",
            "avg. train loss: 0.5494362146167432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 3/200 [01:47<1:57:43, 35.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg test loss : 0.5286262035369873\n",
            "Epoch: 4\n",
            "avg. train loss: 0.4932512489415832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 4/200 [02:23<1:57:03, 35.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg test loss : 0.5042452216148376\n",
            "Epoch: 5\n",
            "avg. train loss: 0.45136655942868376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▎         | 5/200 [02:59<1:56:27, 35.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg test loss : 0.45376306772232056\n",
            "Epoch: 6\n",
            "avg. train loss: 0.4275758231090287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 6/200 [03:35<1:55:57, 35.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg test loss : 0.4232565462589264\n",
            "Epoch: 7\n",
            "avg. train loss: 0.422951975111234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▎         | 7/200 [04:10<1:55:04, 35.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg test loss : 0.4024121165275574\n",
            "Epoch: 8\n",
            "avg. train loss: 0.3869908012576022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 8/200 [04:46<1:54:37, 35.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg test loss : 0.38599854707717896\n",
            "Epoch: 9\n",
            "avg. train loss: 0.3916930389606347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 9/200 [05:22<1:53:56, 35.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg test loss : 0.38792526721954346\n",
            "Epoch: 10\n",
            "avg. train loss: 0.38824278065713785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 10/200 [05:58<1:53:25, 35.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg test loss : 0.38327646255493164\n",
            "Epoch: 11\n",
            "avg. train loss: 0.3653402227466389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 11/200 [06:34<1:52:55, 35.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg test loss : 0.3678523004055023\n",
            "Epoch: 12\n",
            "avg. train loss: 0.3478632185418727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 12/200 [07:09<1:52:09, 35.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg test loss : 0.35358843207359314\n",
            "Epoch: 13\n",
            "avg. train loss: 0.34141655897690076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▋         | 13/200 [07:45<1:51:36, 35.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg test loss : 0.3679891526699066\n",
            "Epoch: 14\n",
            "avg. train loss: 0.3431423518617274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 14/200 [08:21<1:51:08, 35.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg test loss : 0.4059922695159912\n",
            "Epoch: 15\n",
            "avg. train loss: 0.34823477217706583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 15/200 [08:57<1:50:30, 35.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg test loss : 0.3386802673339844\n",
            "Epoch: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxHPfJcfHJTJ"
      },
      "source": [
        "sub= pd.DataFrame({'id': test_ids, 'pressure': predictions})\n",
        "sub.to_csv('submission.csv',index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8yo_vCCHM6Y"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(avg_train_losses)\n",
        "plt.plot(avg_val_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQb_l1wiHOkm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}